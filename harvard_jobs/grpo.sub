#!/bin/bash
#SBATCH --job-name=grpo_n     # create a short name for your job
#SBATCH -c 96                # Number of cores (-c)
#SBATCH -t 12:00:00          # Runtime in D-HH:MM, minimum of 10 minutes
#SBATCH --mem=960000           # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --mail-type=begin        # send email when job begins
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-type=fail         # send email if job fails
#SBATCH --mail-user=zg292@cornell.edu
#SBATCH --gres=gpu:nvidia_h100_80gb_hbm3:4
#SBATCH -o ./runs/output_%j.out  # File to which STDOUT will be written, %j inserts jobid
#SBATCH -e ./runs/errors_%j.err  # File to which STDERR will be written, %j inserts jobid
#SBATCH -p kempner_h100
#SBATCH --account=kempner_kdbrantley_lab

######################
### Set enviroment 
######################

module purge
module load cuda/12.2.0-fasrc01
module load gcc/10.2.0-fasrc01
source /n/sw/Mambaforge-23.11.0-0/etc/profile.d/conda.sh
conda activate zero
export HF_HOME='/n/holylabs/LABS/kdbrantley_lab/Lab/zhaolin'

GPUS_PER_NODE=4
MINI_BATCH_SIZE=128 # number of prompts for each update; each batch we do BATCH_SIZE / MINI_BATCH_SIZE updates
MICRO_BATCH_SIZEPER_DEVICE=1 # reduce to reduce memory but slower
MODEL_SIZE="1.5"
MAX_PROMPT_LENGTH=256
MAX_RESPONSE_LENGTH=1024
DATASET="countdown"
USE_KL_LOSS="True"

######################
### Set Slurm Job
######################
export VLLM_ATTENTION_BACKEND=XFORMERS

num_rollout=(2)
batch_sizes=(512)
lrs=(1e-6)
kl_loss_coef=(1e-3)

for klloss in "${kl_loss_coef[@]}"; do
    for lr in "${lrs[@]}"; do
        for bs in "${batch_sizes[@]}"; do
            for nr in "${num_rollout[@]}"; do
                python3 -m verl.trainer.main_ppo \
                            algorithm.adv_estimator=grpo \
                            data.train_files=/n/holylabs/LABS/kdbrantley_lab/Lab/zhaolin/reasoning/data/${DATASET}/train.parquet \
                            data.val_files=/n/holylabs/LABS/kdbrantley_lab/Lab/zhaolin/reasoning/data/${DATASET}/test.parquet \
                            data.max_prompt_length=$MAX_PROMPT_LENGTH \
                            data.max_response_length=$MAX_RESPONSE_LENGTH \
                            data.train_batch_size=${bs} \
                            data.val_batch_size=500 \
                            actor_rollout_ref.model.path=Qwen/Qwen2.5-${MODEL_SIZE}B \
                            actor_rollout_ref.actor.optim.lr=$lr \
                            actor_rollout_ref.actor.ppo_mini_batch_size=$MINI_BATCH_SIZE \
                            actor_rollout_ref.actor.ppo_micro_batch_size=$(expr $GPUS_PER_NODE \* $MICRO_BATCH_SIZEPER_DEVICE) \
                            actor_rollout_ref.actor.use_kl_loss=${USE_KL_LOSS} \
                            actor_rollout_ref.actor.kl_loss_coef=${klloss} \
                            actor_rollout_ref.actor.kl_loss_type=low_var_kl \
                            actor_rollout_ref.actor.use_dynamic_bsz=True \
                            actor_rollout_ref.actor.ppo_max_token_len_per_gpu=$(((MAX_RESPONSE_LENGTH + MAX_PROMPT_LENGTH) * MICRO_BATCH_SIZEPER_DEVICE)) \
                            actor_rollout_ref.rollout.log_prob_micro_batch_size=$(expr $GPUS_PER_NODE \* $MICRO_BATCH_SIZEPER_DEVICE) \
                            actor_rollout_ref.rollout.tensor_model_parallel_size=$GPUS_PER_NODE \
                            actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
                            actor_rollout_ref.rollout.n=${nr} \
                            actor_rollout_ref.ref.log_prob_micro_batch_size=$(expr $GPUS_PER_NODE \* $MICRO_BATCH_SIZEPER_DEVICE) \
                            trainer.logger=['wandb'] \
                            +trainer.val_before_train=False \
                            trainer.default_hdfs_dir=null \
                            trainer.n_gpus_per_node=$GPUS_PER_NODE \
                            trainer.nnodes=1 \
                            trainer.save_freq=-1 \
                            trainer.test_freq=50 \
                            trainer.project_name=${DATASET} \
                            trainer.experiment_name=qwen2.5-${MODEL_SIZE}b-grpo-n-${nr}-bs-${bs}-lr-${lr}-klloss-${klloss} \
                            trainer.total_epochs=${EPOCH} # 2>&1 | tee verl_demo.log
            done
        done
    done
done